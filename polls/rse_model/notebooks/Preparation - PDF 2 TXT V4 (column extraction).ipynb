{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - parsing paragraphs from all PDFs\n",
    "\n",
    "TOD: rename input file, work on entreprises.csv, do this for all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T15:30:33.280482Z",
     "start_time": "2020-04-04T15:30:33.275495Z"
    }
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# processing imports\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# pdfminer imports\n",
    "from pdfminer.pdfdocument import PDFDocument, PDFNoOutlines\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTPage, LTChar, LTAnno, LAParams, LTTextBox, LTTextLine\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T15:37:14.989340Z",
     "start_time": "2020-04-04T15:37:14.955453Z"
    }
   },
   "outputs": [],
   "source": [
    "# utils elements to move to utils after development\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    '''\n",
    "        For the given path, get the List of all files in the directory tree \n",
    "    '''\n",
    "    paths = []\n",
    "    for path, subdirs, files in os.walk(dirName):\n",
    "        for name in files:\n",
    "            paths.append((Path(path+\"/\"+name)))            \n",
    "    return paths\n",
    "\n",
    "\n",
    "class PDFPageDetailedAggregator(PDFPageAggregator):\n",
    "    def __init__(self, rsrcmgr, pageno=1, laparams=None):\n",
    "        PDFPageAggregator.__init__(self, rsrcmgr, pageno=pageno, laparams=laparams)\n",
    "        self.rows = []\n",
    "        self.page_number = 0\n",
    "    def receive_layout(self, ltpage):        \n",
    "        def render(item, page_number):\n",
    "            if isinstance(item, LTPage) or isinstance(item, LTTextBox):\n",
    "                for child in item:\n",
    "                    render(child, page_number)\n",
    "            elif isinstance(item, LTTextLine):\n",
    "                child_str = ''\n",
    "                for child in item:\n",
    "                    if isinstance(child, (LTChar, LTAnno)):\n",
    "                        child_str += child.get_text()\n",
    "                child_str = ' '.join(child_str.split()).strip()\n",
    "                if child_str:\n",
    "                    row = (page_number, item.bbox[0], item.bbox[1], item.bbox[2], item.bbox[3], child_str) # bbox == (x1, y1, x2, y2)\n",
    "                    self.rows.append(row)\n",
    "                for child in item:\n",
    "                    render(child, page_number)\n",
    "            return\n",
    "        render(ltpage, self.page_number)\n",
    "        self.page_number += 1\n",
    "        self.rows = sorted(self.rows, key = lambda x: (x[0], -x[2]))\n",
    "        self.result = ltpage\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "# TODO: deal with words cut in half \"pro- pagation of...\"\n",
    "def convert(input_file, rse_ranges):\n",
    "    \"\"\"\n",
    "    :param input_file: PDF filename\n",
    "    :param rse_ranges: (nb_first_page_rse:int, nb_last_page_rse:int), starting at 1\n",
    "    \"\"\"\n",
    "    fp = open(input_file, 'rb')\n",
    "    parser = PDFParser(fp)\n",
    "    doc = PDFDocument(parser)\n",
    "    # doc.initialize(\"passwrd\") # leave empty for no password\n",
    "\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageDetailedAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    pages_selection = range(rse_ranges[0]-1,(rse_ranges[1]-1)+1)\n",
    "    for nb_page_parsed, page in enumerate(PDFPage.create_pages(doc)):\n",
    "        if nb_page_parsed in pages_selection:\n",
    "            interpreter.process_page(page)\n",
    "            # receive the LTPage object for this page\n",
    "            device.get_result()\n",
    "\n",
    "    # GROUPING BY COLUMN\n",
    "    column_text = OrderedDict() # keep order is of identification in the document.\n",
    "    for (page_nb, x_min, y_min, _, y_max, text) in device.rows:\n",
    "        page_nb = (pages_selection[0]) + page_nb # elsewise device starts again at 0\n",
    "        if page_nb not in column_text.keys():\n",
    "            column_text[page_nb] = {}\n",
    "        x_group = round(x_min)//50 # Si trois paragraphes -> shift de 170, max Ã  droite ~600\n",
    "        try:\n",
    "            column_text[page_nb][x_group].append((y_min, y_max, text))\n",
    "        except:\n",
    "            column_text[page_nb][x_group] = [(y_min, y_max, text)]\n",
    "\n",
    "    grouped_data_final = []\n",
    "    paragraph_index = 0\n",
    "    \n",
    "    # CREATE THE PARAGRAPHS IN EACH COLUMN\n",
    "    # define minimal conditions to define a change of paragraph:\n",
    "    # Being spaced by more than the size of each line (min if different to accoutn for titles)\n",
    "    for page_nb, x_groups_dict in column_text.items():\n",
    "        for x_group_name, x_groups_data in x_groups_dict.items():\n",
    "            x_groups_data = sorted(x_groups_data, key=lambda x: x[0], reverse=True) # sort vertically, higher y = before\n",
    "            x_groups_data_paragraphs = []\n",
    "            \n",
    "            p = {\"y_min\":x_groups_data[0][0], \n",
    "                 \"y_max\":x_groups_data[0][1],\n",
    "                 \"paragraph\":x_groups_data[0][2]}\n",
    "            previous_height = p[\"y_max\"] - p[\"y_min\"]\n",
    "            for y_min, y_max, paragraph in x_groups_data[1:]:\n",
    "                current_height = y_max - y_min\n",
    "                min_height = min(previous_height,current_height)\n",
    "                \n",
    "                if (p[\"y_min\"]-y_max)<min_height: #paragraph update\n",
    "                    p[\"y_min\"] = y_min\n",
    "                    p[\"paragraph\"] = p[\"paragraph\"] + \" \" + paragraph\n",
    "                else: # break paragraph, start new one\n",
    "                    x_groups_data_paragraphs.append(p)\n",
    "                    p = {\"y_min\":y_min,\n",
    "                         \"y_max\":y_max,\n",
    "                         \"paragraph\":paragraph}\n",
    "                previous_height = current_height\n",
    "            # add the last paragraph of column\n",
    "            x_groups_data_paragraphs.append(p)\n",
    "            # structure the output\n",
    "            for p in x_groups_data_paragraphs:\n",
    "                grouped_data_final.append({\"paragraph_id\":paragraph_index,\n",
    "                                           \"page_nb\":page_nb,\n",
    "                                           \"x_group\":x_group,\n",
    "                                           \"y_min_paragraph\":round(p[\"y_min\"]), \n",
    "                                           \"y_max_paragraph\":round(p[\"y_max\"]), \n",
    "                                           \"paragraph\":p[\"paragraph\"]})\n",
    "                paragraph_index+=1\n",
    "    return grouped_data_final\n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    \"Compares two strings and returns a similarity ratio between 0 and 1\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def cut_footer(Df, p, siren, verbose=False):\n",
    "    \"Cut the paragraph with lowest y_min if other paragraphs are similar\"\n",
    "    \"The similarity is measured with function similar\"\n",
    "    try:\n",
    "        footers=[]\n",
    "        isFooter=True\n",
    "        y_footer = Df[Df['SIREN']==siren]['y_min_paragraph'].min()\n",
    "        while isFooter:        \n",
    "            if len(Df[Df['y_min_paragraph']==y_footer]['paragraph'].values)>1:\n",
    "                footers.append(*Df[Df['y_min_paragraph']==y_footer]['paragraph'].values[:1])\n",
    "                for phrase_1 in Df[Df['y_min_paragraph']==y_footer]['paragraph'].values[1:]:\n",
    "                    if similar(str(footers[-1]), str(phrase_1))<p:\n",
    "                        footers.pop(-1)\n",
    "                        isFooter=False\n",
    "                        break\n",
    "                Df[Df[\"SIREN\"]==siren]=Df[(Df[\"SIREN\"]==siren)&(Df['y_min_paragraph']>y_footer)]\n",
    "            else:\n",
    "                isFooter=False\n",
    "            y_footer = Df[Df['SIREN']==siren]['y_min_paragraph'].min()\n",
    "        \n",
    "        #Below part is for human check that the function works properly\n",
    "        if verbose==True:\n",
    "            print(\"Denomination:\", *Df[Df['SIREN']==siren]['denomination'].unique(), siren)\n",
    "            if footers!=[]:\n",
    "                print(\"Footer(s) --->\", *footers)\n",
    "            print(\"Not footer --->\", \\\n",
    "                  Df[Df['y_min_paragraph']==y_footer]['paragraph'].values[:1][0][:50],\\\n",
    "                  \" - Page\", *Df[Df['y_min_paragraph']==y_footer]['page_nb'].values[:1])\n",
    "    except:\n",
    "        print(\"Error with SIREN =\",siren)\n",
    "    \n",
    "def cut_header(Df, p, siren, verbose=False):\n",
    "    \"Same as function cut_footer() but for headers\"\n",
    "    try:\n",
    "        headers=[]\n",
    "        isHeader=True\n",
    "        y_header = Df[Df['SIREN']==siren]['y_max_paragraph'].max()\n",
    "        while isHeader:        \n",
    "            if len(Df[Df['y_max_paragraph']==y_header]['paragraph'].values)>1:\n",
    "                headers.append(*Df[Df['y_max_paragraph']==y_header]['paragraph'].values[:1])\n",
    "                for phrase_1 in Df[Df['y_max_paragraph']==y_header]['paragraph'].values[1:]:\n",
    "                    if similar(str(headers[-1]), str(phrase_1))<p:\n",
    "                        headers.pop(-1)\n",
    "                        isHeader=False\n",
    "                        break\n",
    "                Df[Df[\"SIREN\"]==siren]=Df[(Df[\"SIREN\"]==siren)&(Df['y_max_paragraph']<y_header)]\n",
    "            else:\n",
    "                isHeader=False\n",
    "            y_header = Df[Df['SIREN']==siren]['y_max_paragraph'].max()\n",
    "        \n",
    "        #Below part is for human check that the function works properly\n",
    "        if verbose==True:\n",
    "            if headers!=[]:\n",
    "                print(\"Header(s) --->\", headers)\n",
    "            print(\"Not header --->\", \\\n",
    "                  Df[Df['y_max_paragraph']==y_header]['paragraph'].values[:1][0][:50], \\\n",
    "                  \" - Page\", *Df[Df['y_max_paragraph']==y_header]['page_nb'].values[:1],'\\n')\n",
    "    except:\n",
    "        print(\"Error with SIREN =\",siren, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T15:46:25.036206Z",
     "start_time": "2020-04-04T15:38:05.686479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2f1756bf49431680741e5b2fcc1a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0/14 michelin [michelin_2018_ddr.pdf]\n",
      "Pages: 204 to 282\n",
      "Processing 1/14 bouygues [bouygues_2018_ddr.pdf]\n",
      "Pages: 98 to 124\n",
      "Processing 2/14 eiffage [eiffage_2018_ddr.pdf]\n",
      "Pages: 125 to 202\n",
      "Processing 3/14 saintgobain [saintgobain_2018_ddr.pdf]\n",
      "Pages: 76 to 79\n",
      "Pages: 101 to 104\n",
      "Pages: 329 to 332\n",
      "Processing 4/14 vinci [vinci_2018_ddr.pdf]\n",
      "Pages: 38 to 48\n",
      "Pages: 207 to 266\n",
      "Processing 5/14 edf [edf_2018_ddr.pdf]\n",
      "Pages: 149 to 236\n",
      "Processing 6/14 engie [engie_2018_ddr.pdf]\n",
      "Pages: 63 to 110\n",
      "Processing 7/14 orano [orano_2018_ddr.pdf]\n",
      "Pages: 61 to 78\n",
      "Processing 8/14 total [total_2018_ddr.pdf]\n",
      "Pages: 179 to 226\n",
      "Processing 9/14 auchanholding [auchanholding_2018_ddr.pdf]\n",
      "Pages: 115 to 165\n",
      "Processing 10/14 carrefour [carrefour_2018_ddr.pdf]\n",
      "Pages: 39 to 130\n",
      "Processing 11/14 casino [casino_2018_dpef.pdf]\n",
      "Pages: 1 to 76\n",
      "Processing 12/14 scaouest [scaouest_2018_dpef.pdf]\n",
      "Pages: 1 to 39\n",
      "Processing 13/14 lvmh [lvmh_2018_dpef.pdf]\n",
      "Pages: 1 to 54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filepaths\n",
    "entreprises_filename = \"../../data/input/Entreprises/entreprises.csv\"\n",
    "input_path = \"../../data/input/DPEFs/\"\n",
    "output_filename= \"../../data/processed/DPEFs/dpef_paragraphs.csv\"\n",
    "\n",
    "# Entreprises : data\n",
    "dict_entreprises = pd.read_csv(entreprises_filename, sep=\";\").set_index(\"project_denomination\").T.to_dict()\n",
    "# Looks like 'vinci': {'SIREN': 552037806, 'denomination': 'VINCI', 'rse_ranges':(38,48)|(207,266)},\n",
    "\n",
    "# DPEF\n",
    "all_input_files = getListOfFiles(input_path)\n",
    "all_input_files = [p for p in all_input_files if p.name.lower().endswith(\".pdf\")]\n",
    "\n",
    "# output has shape...\n",
    "df_parsed_data = pd.DataFrame(columns = [\"SIREN\",\n",
    "                                         \"project_denomination\",\n",
    "                                         \"denomination\",\n",
    "                                         \"pdf_name\", \n",
    "                                         \"page_nb\",\n",
    "                                         \"paragraph_id\", # x_min for now\n",
    "                                         \"paragraph\",\n",
    "                                         \"x_group\",\n",
    "                                         \"y_min_paragraph\",\n",
    "                                         \"y_max_paragraph\"])\n",
    "for i, input_file in tqdm(enumerate(all_input_files)):\n",
    "    if input_file.name.endswith(\"pdf\"):\n",
    "        project_denomination = input_file.name.split(\"\\\\\")[-1].split(\"_\")[0] # first word of pdf name\n",
    "        print(\"Processing {}/{} {} [{}]\".format(i+1,len(all_input_files),project_denomination, input_file.name))\n",
    "        for rse_ranges in dict_entreprises[project_denomination][\"rse_ranges\"].split(\"|\"):\n",
    "            rse_ranges = eval(rse_ranges) # tuple format str to actual tuple\n",
    "            print(\"Pages: {} to {}\".format(rse_ranges[0], rse_ranges[1]))\n",
    "            grouped_data_final = convert(input_file, rse_ranges)\n",
    "            for paragraph_data in grouped_data_final:\n",
    "                df_update = {\"SIREN\": dict_entreprises[project_denomination][\"SIREN\"],\n",
    "                             \"denomination\": dict_entreprises[project_denomination][\"denomination\"],\n",
    "                             \"project_denomination\": project_denomination,\n",
    "                             \"pdf_name\": input_file.name.split(\"\\\\\")[-1]}\n",
    "                df_update.update(paragraph_data)\n",
    "                df_parsed_data = df_parsed_data.append(df_update, ignore_index=True)\n",
    "#         break\n",
    "\n",
    "for siren in df_parsed_data[\"SIREN\"].unique():\n",
    "    cut_footer(df_parsed_data, 0.7, siren)\n",
    "    cut_header(df_parsed_data, 0.7, siren)\n",
    "df_parsed_data = df_parsed_data[df_parsed_data.paragraph.notna()] # empty lines created by cuter ?\n",
    "df_parsed_data.to_csv(output_filename,sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parsed_data.groupby(\"project_denomination\").size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
